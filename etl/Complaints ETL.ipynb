{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complainants / Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"by complaintant\" dataset\n",
    "comdata = pd.read_csv(\"../data/raw/Complaints/COPA_Cases_-_By_Complainant_or_Subject.csv\", \\\n",
    "                   dtype={\"LOG_NO\":str,\"CASE_TYPE\":str}) \\\n",
    "        .assign(DATETIME = lambda x: pd.to_datetime(x.COMPLAINT_DATE, format = \"%m/%d/%Y %H:%M:%S %p\")) \\\n",
    "        .assign(COMPLAINT_YEAR = lambda x: x.DATETIME.dt.year)\n",
    "comdata['LOG_NO'] = comdata['LOG_NO'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Beat category\n",
    "comdata = comdata.assign(BEAT = pd.to_numeric(comdata.BEAT.str.strip(), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean capital/lowercase typo in race category:\n",
    "comdata.RACE_OF_COMPLAINANT.replace(to_replace=\"Hispanic, Latino, or Spanish origin\", \\\n",
    "                                   value=\"Hispanic, Latino, or Spanish Origin\", \\\n",
    "                                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Police Shooting to numeric\n",
    "comdata = comdata.assign(POLICE_SHOOTING = lambda x: x.POLICE_SHOOTING.map({'No':0.0, 'Yes':1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Keeping these duplicates: they may be legitimate multi-party complaints\n",
    "# Discard literal duplicates\n",
    "# comdata.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all time columns except year, which is needed for aggregation\n",
    "comdata.drop(labels=['COMPLAINT_DATE', 'COMPLAINT_HOUR', \\\n",
    "                     'COMPLAINT_DAY', 'COMPLAINT_MONTH', 'DATETIME'], \\\n",
    "             axis='columns', \\\n",
    "            inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question of which cases move through the system and how quickly is interesting, \n",
    "# but since we got rid of the \"time\" axis, its easier to assume all cases are \"Closed\" for now\n",
    "comdata.drop(labels='CURRENT_STATUS', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For similar reasons, let's drop assignment until we read up on who sees which kind of cases\n",
    "comdata.drop(labels='ASSIGNMENT', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For similar reasons, let's drop case_type until we decide we want to include these procedural details\n",
    "comdata.drop(labels='CASE_TYPE', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Keeping these duplicates: they may be legitimate multi-party complaints\n",
    "\n",
    "# Clean up duplicates\n",
    "# logcounts = comdata['LOG_NO'].value_counts()\n",
    "# duplogs = logcounts[logcounts > 1].index\n",
    "\n",
    "# def clean_updated_unknowns(data, column_name, dups):\n",
    "#     is_unknown = data[column_name] == 'Unknown'\n",
    "#     is_dup = data['LOG_NO'].isin(dups)\n",
    "#     return data[~(is_unknown & is_dup)]\n",
    "\n",
    "# for col in comdata.columns:\n",
    "#     comdata = clean_updated_unknowns(comdata, col, duplogs)\n",
    "    \n",
    "# Clean up duplicates\n",
    "# logcounts = comdata['LOG_NO'].value_counts()\n",
    "# duplogs = logcounts[logcounts > 1].index\n",
    "# comdata[comdata['LOG_NO'] == duplogs[45]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new indicator columns for the multi-categorical columns\n",
    "\n",
    "def listcolumn_pivot_wider(data, column_name, prefix):\n",
    "    '''\n",
    "    Split a pipe-separated multi-valued string column into dummy variables. \n",
    "    Append dummies to tbl and remove original column.\n",
    "    '''\n",
    "    cleaned = data[column_name].str.replace(\" \", \"\", regex=False)\n",
    "    dummies = cleaned.str.get_dummies()\n",
    "    dummies.rename(columns=lambda c: prefix + c, inplace=True)\n",
    "    data.drop(labels=column_name, axis='columns', inplace=True)\n",
    "    return pd.concat([data, dummies], axis=1)\n",
    "\n",
    "\n",
    "# For simplicity, we are reducing our feature-set to counts by race: don't create the other categorical features\n",
    "comdata = listcolumn_pivot_wider(comdata, 'RACE_OF_COMPLAINANT', 'COMPLAINANT_RACE_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'CURRENT_CATEGORY', 'COMPLAINT_CAT_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'FINDING_CODE', 'COMPLAINT_FINDING_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'SEX_OF_COMPLAINANT', 'COMPLAINANT_SEX_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'AGE_OF_COMPLAINANT', 'COMPLAINANT_AGE_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we are reducing our feature-set to counts by race: drop the other categorical feature columns\n",
    "comdata.drop(labels=['CURRENT_CATEGORY','FINDING_CODE','SEX_OF_COMPLAINANT','AGE_OF_COMPLAINANT'], \\\n",
    "             axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we are limiting race to white/black/latino-other\n",
    "race_columns = comdata.columns.str.contains('COMPLAINANT_RACE')\n",
    "white_columns = comdata.columns.str.contains('White')\n",
    "black_columns = comdata.columns.str.contains('Black')\n",
    "latino_columns = comdata.columns.str.contains('Latino')\n",
    "other_columns = np.logical_and(race_columns, np.logical_not(\\\n",
    "                                             np.logical_or(latino_columns, \\\n",
    "                                             np.logical_or(white_columns, black_columns))))\n",
    "other_columns = comdata.columns[other_columns]\n",
    "comdata = comdata.assign(COMPLAINANT_RACE_Other = sum([comdata[c] for c in other_columns]))\n",
    "comdata.drop(labels=other_columns, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-85cd27d9e3ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pivot \"beats\" column longer: create more rows when complaint spans beats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcomdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BEAT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\s*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                  \u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BEAT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BEAT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5459\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5460\u001b[0m         ):\n\u001b[0;32m-> 5461\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/strings/accessor.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only 2016-2019, the common years from other datasets\n",
    "comdata = comdata.loc[np.logical_and(comdata.COMPLAINT_YEAR >= 2016, comdata.COMPLAINT_YEAR <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "comdata_agg = comdata.drop(labels='LOG_NO', axis='columns').groupby(by=['BEAT','COMPLAINT_YEAR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "!mkdir -p ../data/features\n",
    "comdata_agg.to_csv(\"../data/features/complaints.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Officers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eric/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Load \"by officer\" dataset\n",
    "offdata = pd.read_csv(\"../data/raw/Complaints/COPA_Cases_-_By_Involved_Officer.csv\", \\\n",
    "                      dtype={\"LOG_NO\":str, \"CASE_TYPE\":str}) \\\n",
    "            .assign(DATETIME = lambda x: pd.to_datetime(x.COMPLAINT_DATE, format = \"%m/%d/%Y %H:%M:%S %p\")) \\\n",
    "            .assign(COMPLAINT_YEAR = lambda x: x.DATETIME.dt.year)\n",
    "offdata['LOG_NO'] = offdata['LOG_NO'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Commented out because these may be multi-party complaints\n",
    "# Drop literal duplicates\n",
    "# offdata.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all time columns except year, which is needed for aggregation\n",
    "offdata.drop(labels=['COMPLAINT_DATE', 'COMPLAINT_HOUR', \\\n",
    "                     'COMPLAINT_DAY', 'COMPLAINT_MONTH', 'DATETIME'], \\\n",
    "             axis='columns', \\\n",
    "            inplace = True)\n",
    "\n",
    "# The question of which cases move through the system and how quickly is interesting, \n",
    "# but since we got rid of the \"time\" axis, its easier to assume all cases are \"Closed\" for now\n",
    "offdata.drop(labels='CURRENT_STATUS', axis='columns', inplace=True)\n",
    "\n",
    "# For similar reasons, let's drop assignment until we read up on who sees which kind of cases\n",
    "offdata.drop(labels='ASSIGNMENT', axis='columns', inplace=True)\n",
    "\n",
    "# For similar reasons, let's drop case_type until we decide we want to include these procedural details\n",
    "offdata.drop(labels='CASE_TYPE', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge into by-complainant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpp_venv",
   "language": "python",
   "name": "mlpp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
