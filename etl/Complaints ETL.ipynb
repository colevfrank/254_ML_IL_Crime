{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complainants / Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"by complaintant\" dataset\n",
    "comdata = pd.read_csv(\"../data/raw/Complaints/COPA_Cases_-_By_Complainant_or_Subject.csv\", \\\n",
    "                   dtype={\"LOG_NO\":str,\"CASE_TYPE\":str}) \\\n",
    "        .assign(DATETIME = lambda x: pd.to_datetime(x.COMPLAINT_DATE, format = \"%m/%d/%Y %H:%M:%S %p\")) \\\n",
    "        .assign(COMPLAINT_YEAR = lambda x: x.DATETIME.dt.year)\n",
    "comdata['LOG_NO'] = comdata['LOG_NO'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot \"beats\" column longer: create more rows when complaint spans beats\n",
    "comdata = comdata.assign(BEAT = comdata['BEAT'].str.replace(\"\\s*\", \"\", regex=True)) \\\n",
    "                 .assign(BEAT = comdata['BEAT'].str.split(\"|\")) \\\n",
    "                 .explode('BEAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Beat category\n",
    "comdata = comdata.assign(BEAT = pd.to_numeric(comdata.BEAT.str.strip(), errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean capital/lowercase typo in race category:\n",
    "comdata.RACE_OF_COMPLAINANT.replace(to_replace=\"Hispanic, Latino, or Spanish origin\", \\\n",
    "                                   value=\"Hispanic, Latino, or Spanish Origin\", \\\n",
    "                                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Police Shooting to numeric\n",
    "comdata = comdata.assign(POLICE_SHOOTING = lambda x: x.POLICE_SHOOTING.map({'No':0.0, 'Yes':1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Keeping these duplicates: they may be legitimate multi-party complaints\n",
    "# Discard literal duplicates\n",
    "# comdata.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all time columns except year, which is needed for aggregation\n",
    "comdata.drop(labels=['COMPLAINT_DATE', 'COMPLAINT_HOUR', \\\n",
    "                     'COMPLAINT_DAY', 'COMPLAINT_MONTH', 'DATETIME'], \\\n",
    "             axis='columns', \\\n",
    "            inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question of which cases move through the system and how quickly is interesting, \n",
    "# but since we got rid of the \"time\" axis, its easier to assume all cases are \"Closed\" for now\n",
    "comdata.drop(labels='CURRENT_STATUS', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For similar reasons, let's drop assignment until we read up on who sees which kind of cases\n",
    "comdata.drop(labels='ASSIGNMENT', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For similar reasons, let's drop case_type until we decide we want to include these procedural details\n",
    "comdata.drop(labels='CASE_TYPE', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Keeping these duplicates: they may be legitimate multi-party complaints\n",
    "\n",
    "# Clean up duplicates\n",
    "# logcounts = comdata['LOG_NO'].value_counts()\n",
    "# duplogs = logcounts[logcounts > 1].index\n",
    "\n",
    "# def clean_updated_unknowns(data, column_name, dups):\n",
    "#     is_unknown = data[column_name] == 'Unknown'\n",
    "#     is_dup = data['LOG_NO'].isin(dups)\n",
    "#     return data[~(is_unknown & is_dup)]\n",
    "\n",
    "# for col in comdata.columns:\n",
    "#     comdata = clean_updated_unknowns(comdata, col, duplogs)\n",
    "    \n",
    "# Clean up duplicates\n",
    "# logcounts = comdata['LOG_NO'].value_counts()\n",
    "# duplogs = logcounts[logcounts > 1].index\n",
    "# comdata[comdata['LOG_NO'] == duplogs[45]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new indicator columns for the multi-categorical columns\n",
    "\n",
    "def listcolumn_pivot_wider(data, column_name, prefix):\n",
    "    '''\n",
    "    Split a pipe-separated multi-valued string column into dummy variables. \n",
    "    Append dummies to tbl and remove original column.\n",
    "    '''\n",
    "    cleaned = data[column_name].str.replace(\" \", \"\", regex=False)\n",
    "    dummies = cleaned.str.get_dummies()\n",
    "    dummies.rename(columns=lambda c: prefix + c, inplace=True)\n",
    "    data.drop(labels=column_name, axis='columns', inplace=True)\n",
    "    return pd.concat([data, dummies], axis=1)\n",
    "\n",
    "\n",
    "# For simplicity, we are reducing our feature-set to counts by race: don't create the other categorical features\n",
    "comdata = listcolumn_pivot_wider(comdata, 'RACE_OF_COMPLAINANT', 'COMPLAINANT_RACE_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'CURRENT_CATEGORY', 'COMPLAINT_CAT_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'FINDING_CODE', 'COMPLAINT_FINDING_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'SEX_OF_COMPLAINANT', 'COMPLAINANT_SEX_')\n",
    "# comdata = listcolumn_pivot_wider(comdata, 'AGE_OF_COMPLAINANT', 'COMPLAINANT_AGE_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we are reducing our feature-set to counts by race: drop the other categorical feature columns\n",
    "comdata.drop(labels=['CURRENT_CATEGORY','FINDING_CODE','SEX_OF_COMPLAINANT','AGE_OF_COMPLAINANT'], \\\n",
    "             axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we are limiting race to white/black/latino-other\n",
    "race_columns = comdata.columns.str.contains('COMPLAINANT_RACE')\n",
    "white_columns = comdata.columns.str.contains('White')\n",
    "black_columns = comdata.columns.str.contains('Black')\n",
    "latino_columns = comdata.columns.str.contains('Latino')\n",
    "other_columns = np.logical_and(race_columns, np.logical_not(\\\n",
    "                                             np.logical_or(latino_columns, \\\n",
    "                                             np.logical_or(white_columns, black_columns))))\n",
    "other_columns = comdata.columns[other_columns]\n",
    "comdata = comdata.assign(COMPLAINANT_RACE_Other = sum([comdata[c] for c in other_columns]))\n",
    "comdata.drop(labels=other_columns, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only 2016-2019, the common years from other datasets\n",
    "comdata = comdata.loc[np.logical_and(comdata.COMPLAINT_YEAR >= 2016, comdata.COMPLAINT_YEAR <= 2019)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate\n",
    "comdata_agg = comdata.drop(labels='LOG_NO', axis='columns').groupby(by=['BEAT','COMPLAINT_YEAR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to disk\n",
    "!mkdir -p ../data/features\n",
    "comdata_agg.to_csv(\"../data/features/complaints.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Officers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eric/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Load \"by officer\" dataset\n",
    "offdata = pd.read_csv(\"../data/raw/Complaints/COPA_Cases_-_By_Involved_Officer.csv\", \\\n",
    "                      dtype={\"LOG_NO\":str, \"CASE_TYPE\":str}) \\\n",
    "            .assign(DATETIME = lambda x: pd.to_datetime(x.COMPLAINT_DATE, format = \"%m/%d/%Y %H:%M:%S %p\")) \\\n",
    "            .assign(COMPLAINT_YEAR = lambda x: x.DATETIME.dt.year)\n",
    "offdata['LOG_NO'] = offdata['LOG_NO'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: Commented out because these may be multi-party complaints\n",
    "# Drop literal duplicates\n",
    "# offdata.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all time columns except year, which is needed for aggregation\n",
    "offdata.drop(labels=['COMPLAINT_DATE', 'COMPLAINT_HOUR', \\\n",
    "                     'COMPLAINT_DAY', 'COMPLAINT_MONTH', 'DATETIME'], \\\n",
    "             axis='columns', \\\n",
    "            inplace = True)\n",
    "\n",
    "# The question of which cases move through the system and how quickly is interesting, \n",
    "# but since we got rid of the \"time\" axis, its easier to assume all cases are \"Closed\" for now\n",
    "offdata.drop(labels='CURRENT_STATUS', axis='columns', inplace=True)\n",
    "\n",
    "# For similar reasons, let's drop assignment until we read up on who sees which kind of cases\n",
    "offdata.drop(labels='ASSIGNMENT', axis='columns', inplace=True)\n",
    "\n",
    "# For similar reasons, let's drop case_type until we decide we want to include these procedural details\n",
    "offdata.drop(labels='CASE_TYPE', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge into by-complainant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpp_venv",
   "language": "python",
   "name": "mlpp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
